!pip install nltk


import nltk
nltk.download('punkt')  


import nltk
nltk.data.path.append('Desktop')


from nltk.tokenize import sent_tokenize
text="""Hello Mr.smith,how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard"""
tokenized_text = sent_tokenize(text)
print(tokenized_text)


from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word


from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
print(fdist)


fdist.most_common(2)


import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()



nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)



from nltk.tokenize import word_tokenize
text1="""Hello Mr.smith,how are you doing today?"""
tokenized_sent=word_tokenize(text1)
print(tokenized_sent)
filtered_sent=[]
for w in tokenized_sent:
    if w not in stop_words:
        filtered_sent.append(w)
print("Tokenized Sentences:",tokenized_sent)
print("Filtered Sentence:",filtered_sent)




from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(ps.stem(w))
    
print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",stemmed_words)




nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()

from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()

word = "flying"
print("Lemmenized word:",lem.lemmatize(word,"v"))
print("Stemmed word:",stem.stem(word))




import nltk
nltk.download('punkt')  # Required for word_tokenize

from nltk.tokenize import word_tokenize

# Define the sentence
sent = "The sky is blue and the weather is nice."

# Tokenize the sentence
tokens = word_tokenize(sent)
print(tokens)




sent = "Albert Einstein was born in Ulm,Germant in 1879."




tokens=nltk.word_tokenize(sent)
print(tokens)




nltk.download('averaged_perceptron_tagger')
nltk.pos_tag(tokens)




from sklearn.feature_extraction.text import TfidfVectorizer





corpus = [
    "Sachin was the GOAT of the previous generation",
    "Virat is the GOAT of the this generation",
    "Shubman will be the GOAT of the next generation"
]
vectorizer = TfidfVectorizer()





matrix = vectorizer.fit(corpus)
matrix.vocabulary_





tfidf_matrix = vectorizer.transform(corpus)
print(tfidf_matrix)





print(vectorizer.get_feature_names_out())